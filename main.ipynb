{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42766 entries, 0 to 42765\n",
      "Data columns (total 73 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   nr. sessions                 42766 non-null  float64\n",
      " 1   total km                     42766 non-null  float64\n",
      " 2   km Z3-4                      42766 non-null  float64\n",
      " 3   km Z5-T1-T2                  42766 non-null  float64\n",
      " 4   km sprinting                 42766 non-null  float64\n",
      " 5   strength training            42766 non-null  float64\n",
      " 6   hours alternative            42766 non-null  float64\n",
      " 7   perceived exertion           42766 non-null  float64\n",
      " 8   perceived trainingSuccess    42766 non-null  float64\n",
      " 9   perceived recovery           42766 non-null  float64\n",
      " 10  nr. sessions.1               42766 non-null  float64\n",
      " 11  total km.1                   42766 non-null  float64\n",
      " 12  km Z3-4.1                    42766 non-null  float64\n",
      " 13  km Z5-T1-T2.1                42766 non-null  float64\n",
      " 14  km sprinting.1               42766 non-null  float64\n",
      " 15  strength training.1          42766 non-null  float64\n",
      " 16  hours alternative.1          42766 non-null  float64\n",
      " 17  perceived exertion.1         42766 non-null  float64\n",
      " 18  perceived trainingSuccess.1  42766 non-null  float64\n",
      " 19  perceived recovery.1         42766 non-null  float64\n",
      " 20  nr. sessions.2               42766 non-null  float64\n",
      " 21  total km.2                   42766 non-null  float64\n",
      " 22  km Z3-4.2                    42766 non-null  float64\n",
      " 23  km Z5-T1-T2.2                42766 non-null  float64\n",
      " 24  km sprinting.2               42766 non-null  float64\n",
      " 25  strength training.2          42766 non-null  float64\n",
      " 26  hours alternative.2          42766 non-null  float64\n",
      " 27  perceived exertion.2         42766 non-null  float64\n",
      " 28  perceived trainingSuccess.2  42766 non-null  float64\n",
      " 29  perceived recovery.2         42766 non-null  float64\n",
      " 30  nr. sessions.3               42766 non-null  float64\n",
      " 31  total km.3                   42766 non-null  float64\n",
      " 32  km Z3-4.3                    42766 non-null  float64\n",
      " 33  km Z5-T1-T2.3                42766 non-null  float64\n",
      " 34  km sprinting.3               42766 non-null  float64\n",
      " 35  strength training.3          42766 non-null  float64\n",
      " 36  hours alternative.3          42766 non-null  float64\n",
      " 37  perceived exertion.3         42766 non-null  float64\n",
      " 38  perceived trainingSuccess.3  42766 non-null  float64\n",
      " 39  perceived recovery.3         42766 non-null  float64\n",
      " 40  nr. sessions.4               42766 non-null  float64\n",
      " 41  total km.4                   42766 non-null  float64\n",
      " 42  km Z3-4.4                    42766 non-null  float64\n",
      " 43  km Z5-T1-T2.4                42766 non-null  float64\n",
      " 44  km sprinting.4               42766 non-null  float64\n",
      " 45  strength training.4          42766 non-null  float64\n",
      " 46  hours alternative.4          42766 non-null  float64\n",
      " 47  perceived exertion.4         42766 non-null  float64\n",
      " 48  perceived trainingSuccess.4  42766 non-null  float64\n",
      " 49  perceived recovery.4         42766 non-null  float64\n",
      " 50  nr. sessions.5               42766 non-null  float64\n",
      " 51  total km.5                   42766 non-null  float64\n",
      " 52  km Z3-4.5                    42766 non-null  float64\n",
      " 53  km Z5-T1-T2.5                42766 non-null  float64\n",
      " 54  km sprinting.5               42766 non-null  float64\n",
      " 55  strength training.5          42766 non-null  float64\n",
      " 56  hours alternative.5          42766 non-null  float64\n",
      " 57  perceived exertion.5         42766 non-null  float64\n",
      " 58  perceived trainingSuccess.5  42766 non-null  float64\n",
      " 59  perceived recovery.5         42766 non-null  float64\n",
      " 60  nr. sessions.6               42766 non-null  float64\n",
      " 61  total km.6                   42766 non-null  float64\n",
      " 62  km Z3-4.6                    42766 non-null  float64\n",
      " 63  km Z5-T1-T2.6                42766 non-null  float64\n",
      " 64  km sprinting.6               42766 non-null  float64\n",
      " 65  strength training.6          42766 non-null  float64\n",
      " 66  hours alternative.6          42766 non-null  float64\n",
      " 67  perceived exertion.6         42766 non-null  float64\n",
      " 68  perceived trainingSuccess.6  42766 non-null  float64\n",
      " 69  perceived recovery.6         42766 non-null  float64\n",
      " 70  Athlete ID                   42766 non-null  int64  \n",
      " 71  injury                       42766 non-null  int64  \n",
      " 72  Date                         42766 non-null  int64  \n",
      "dtypes: float64(70), int64(3)\n",
      "memory usage: 23.8 MB\n",
      "Features with high correlation with the target variable:\n",
      "perceived exertion             0.039748\n",
      "perceived exertion.4           0.037525\n",
      "perceived exertion.5           0.037154\n",
      "perceived exertion.2           0.036932\n",
      "perceived trainingSuccess.4    0.034931\n",
      "                                 ...   \n",
      "hours alternative.2            0.001365\n",
      "km Z3-4                        0.000663\n",
      "km Z3-4.3                      0.000592\n",
      "km Z5-T1-T2.1                  0.000493\n",
      "hours alternative.3            0.000461\n",
      "Name: injury, Length: 72, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "day_ds = pd.read_csv('day_approach_maskedID_timeseries.csv')\n",
    "day_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high correlation with the target variable:\n",
      "perceived exertion             0.039748\n",
      "perceived exertion.4           0.037525\n",
      "perceived exertion.5           0.037154\n",
      "perceived exertion.2           0.036932\n",
      "perceived trainingSuccess.4    0.034931\n",
      "                                 ...   \n",
      "hours alternative.2            0.001365\n",
      "km Z3-4                        0.000663\n",
      "km Z3-4.3                      0.000592\n",
      "km Z5-T1-T2.1                  0.000493\n",
      "hours alternative.3            0.000461\n",
      "Name: injury, Length: 72, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'injury'\n",
    "\n",
    "# Compute correlation coefficients\n",
    "correlation_matrix = day_ds.corr()\n",
    "\n",
    "# Extract correlation coefficients with the target variable\n",
    "correlation_with_target = correlation_matrix[target_variable].drop(target_variable)\n",
    "\n",
    "# Sort the correlation coefficients in descending order\n",
    "correlation_with_target_sorted = correlation_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "# Print the features with high correlation with the target variable\n",
    "print(\"Features with high correlation with the target variable:\")\n",
    "correlation_with_target_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42798 entries, 0 to 42797\n",
      "Data columns (total 72 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   nr. sessions                                   42798 non-null  float64\n",
      " 1   nr. rest days                                  42798 non-null  float64\n",
      " 2   total kms                                      42798 non-null  float64\n",
      " 3   max km one day                                 42798 non-null  float64\n",
      " 4   total km Z3-Z4-Z5-T1-T2                        42798 non-null  float64\n",
      " 5   nr. tough sessions (effort in Z5, T1 or T2)    42798 non-null  float64\n",
      " 6   nr. days with interval session                 42798 non-null  float64\n",
      " 7   total km Z3-4                                  42798 non-null  float64\n",
      " 8   max km Z3-4 one day                            42798 non-null  float64\n",
      " 9   total km Z5-T1-T2                              42798 non-null  float64\n",
      " 10  max km Z5-T1-T2 one day                        42798 non-null  float64\n",
      " 11  total hours alternative training               42798 non-null  float64\n",
      " 12  nr. strength trainings                         42798 non-null  float64\n",
      " 13  avg exertion                                   42798 non-null  float64\n",
      " 14  min exertion                                   42798 non-null  float64\n",
      " 15  max exertion                                   42798 non-null  float64\n",
      " 16  avg training success                           42798 non-null  float64\n",
      " 17  min training success                           42798 non-null  float64\n",
      " 18  max training success                           42798 non-null  float64\n",
      " 19  avg recovery                                   42798 non-null  float64\n",
      " 20  min recovery                                   42798 non-null  float64\n",
      " 21  max recovery                                   42798 non-null  float64\n",
      " 22  nr. sessions.1                                 42798 non-null  float64\n",
      " 23  nr. rest days.1                                42798 non-null  float64\n",
      " 24  total kms.1                                    42798 non-null  float64\n",
      " 25  max km one day.1                               42798 non-null  float64\n",
      " 26  total km Z3-Z4-Z5-T1-T2.1                      42798 non-null  float64\n",
      " 27  nr. tough sessions (effort in Z5, T1 or T2).1  42798 non-null  float64\n",
      " 28  nr. days with interval session.1               42798 non-null  float64\n",
      " 29  total km Z3-4.1                                42798 non-null  float64\n",
      " 30  max km Z3-4 one day.1                          42798 non-null  float64\n",
      " 31  total km Z5-T1-T2.1                            42798 non-null  float64\n",
      " 32  max km Z5-T1-T2 one day.1                      42798 non-null  float64\n",
      " 33  total hours alternative training.1             42798 non-null  float64\n",
      " 34  nr. strength trainings.1                       42798 non-null  float64\n",
      " 35  avg exertion.1                                 42798 non-null  float64\n",
      " 36  min exertion.1                                 42798 non-null  float64\n",
      " 37  max exertion.1                                 42798 non-null  float64\n",
      " 38  avg training success.1                         42798 non-null  float64\n",
      " 39  min training success.1                         42798 non-null  float64\n",
      " 40  max training success.1                         42798 non-null  float64\n",
      " 41  avg recovery.1                                 42798 non-null  float64\n",
      " 42  min recovery.1                                 42798 non-null  float64\n",
      " 43  max recovery.1                                 42798 non-null  float64\n",
      " 44  nr. sessions.2                                 42798 non-null  float64\n",
      " 45  nr. rest days.2                                42798 non-null  float64\n",
      " 46  total kms.2                                    42798 non-null  float64\n",
      " 47  max km one day.2                               42798 non-null  float64\n",
      " 48  total km Z3-Z4-Z5-T1-T2.2                      42798 non-null  float64\n",
      " 49  nr. tough sessions (effort in Z5, T1 or T2).2  42798 non-null  float64\n",
      " 50  nr. days with interval session.2               42798 non-null  float64\n",
      " 51  total km Z3-4.2                                42798 non-null  float64\n",
      " 52  max km Z3-4 one day.2                          42798 non-null  float64\n",
      " 53  total km Z5-T1-T2.2                            42798 non-null  float64\n",
      " 54  max km Z5-T1-T2 one day.2                      42798 non-null  float64\n",
      " 55  total hours alternative training.2             42798 non-null  float64\n",
      " 56  nr. strength trainings.2                       42798 non-null  float64\n",
      " 57  avg exertion.2                                 42798 non-null  float64\n",
      " 58  min exertion.2                                 42798 non-null  float64\n",
      " 59  max exertion.2                                 42798 non-null  float64\n",
      " 60  avg training success.2                         42798 non-null  float64\n",
      " 61  min training success.2                         42798 non-null  float64\n",
      " 62  max training success.2                         42798 non-null  float64\n",
      " 63  avg recovery.2                                 42798 non-null  float64\n",
      " 64  min recovery.2                                 42798 non-null  float64\n",
      " 65  max recovery.2                                 42798 non-null  float64\n",
      " 66  Athlete ID                                     42798 non-null  int64  \n",
      " 67  injury                                         42798 non-null  int64  \n",
      " 68  rel total kms week 0_1                         42798 non-null  float64\n",
      " 69  rel total kms week 0_2                         42798 non-null  float64\n",
      " 70  rel total kms week 1_2                         42798 non-null  float64\n",
      " 71  Date                                           42798 non-null  int64  \n",
      "dtypes: float64(69), int64(3)\n",
      "memory usage: 23.5 MB\n"
     ]
    }
   ],
   "source": [
    "week_ds=pd.read_csv('week_approach_maskedID_timeseries.csv')\n",
    "week_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nr. sessions</th>\n",
       "      <th>total km</th>\n",
       "      <th>km Z3-4</th>\n",
       "      <th>km Z5-T1-T2</th>\n",
       "      <th>km sprinting</th>\n",
       "      <th>strength training</th>\n",
       "      <th>hours alternative</th>\n",
       "      <th>perceived exertion</th>\n",
       "      <th>perceived trainingSuccess</th>\n",
       "      <th>perceived recovery</th>\n",
       "      <th>...</th>\n",
       "      <th>km Z5-T1-T2.6</th>\n",
       "      <th>km sprinting.6</th>\n",
       "      <th>strength training.6</th>\n",
       "      <th>hours alternative.6</th>\n",
       "      <th>perceived exertion.6</th>\n",
       "      <th>perceived trainingSuccess.6</th>\n",
       "      <th>perceived recovery.6</th>\n",
       "      <th>Athlete ID</th>\n",
       "      <th>injury</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "      <td>42766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.829561</td>\n",
       "      <td>7.038187</td>\n",
       "      <td>0.691381</td>\n",
       "      <td>0.579930</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>0.116237</td>\n",
       "      <td>0.163492</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.195898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580347</td>\n",
       "      <td>0.072595</td>\n",
       "      <td>0.116120</td>\n",
       "      <td>0.162308</td>\n",
       "      <td>0.247550</td>\n",
       "      <td>0.349503</td>\n",
       "      <td>0.196224</td>\n",
       "      <td>34.550858</td>\n",
       "      <td>0.013632</td>\n",
       "      <td>1228.039892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.580696</td>\n",
       "      <td>7.473216</td>\n",
       "      <td>2.317657</td>\n",
       "      <td>1.811938</td>\n",
       "      <td>0.483480</td>\n",
       "      <td>0.326010</td>\n",
       "      <td>0.549664</td>\n",
       "      <td>0.257262</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.190321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.814538</td>\n",
       "      <td>0.483691</td>\n",
       "      <td>0.326016</td>\n",
       "      <td>0.554031</td>\n",
       "      <td>0.256718</td>\n",
       "      <td>0.368042</td>\n",
       "      <td>0.190568</td>\n",
       "      <td>19.050033</td>\n",
       "      <td>0.115960</td>\n",
       "      <td>807.021168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>436.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1913.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.900000</td>\n",
       "      <td>42.200000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2673.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nr. sessions      total km       km Z3-4   km Z5-T1-T2  km sprinting  \\\n",
       "count  42766.000000  42766.000000  42766.000000  42766.000000  42766.000000   \n",
       "mean       0.829561      7.038187      0.691381      0.579930      0.073016   \n",
       "std        0.580696      7.473216      2.317657      1.811938      0.483480   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      6.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000     12.000000      0.000000      0.000000      0.000000   \n",
       "max        2.000000     55.900000     42.200000     48.000000     40.000000   \n",
       "\n",
       "       strength training  hours alternative  perceived exertion  \\\n",
       "count       42766.000000       42766.000000        42766.000000   \n",
       "mean            0.116237           0.163492            0.247788   \n",
       "std             0.326010           0.549664            0.257262   \n",
       "min             0.000000           0.000000           -0.010000   \n",
       "25%             0.000000           0.000000           -0.010000   \n",
       "50%             0.000000           0.000000            0.160000   \n",
       "75%             0.000000           0.000000            0.440000   \n",
       "max             2.000000          10.220000            1.000000   \n",
       "\n",
       "       perceived trainingSuccess  perceived recovery  ...  km Z5-T1-T2.6  \\\n",
       "count               42766.000000        42766.000000  ...   42766.000000   \n",
       "mean                    0.349802            0.195898  ...       0.580347   \n",
       "std                     0.368300            0.190321  ...       1.814538   \n",
       "min                    -0.010000           -0.010000  ...       0.000000   \n",
       "25%                    -0.010000           -0.010000  ...       0.000000   \n",
       "50%                     0.260000            0.160000  ...       0.000000   \n",
       "75%                     0.720000            0.300000  ...       0.000000   \n",
       "max                     1.000000            1.000000  ...      48.000000   \n",
       "\n",
       "       km sprinting.6  strength training.6  hours alternative.6  \\\n",
       "count    42766.000000         42766.000000         42766.000000   \n",
       "mean         0.072595             0.116120             0.162308   \n",
       "std          0.483691             0.326016             0.554031   \n",
       "min          0.000000             0.000000             0.000000   \n",
       "25%          0.000000             0.000000             0.000000   \n",
       "50%          0.000000             0.000000             0.000000   \n",
       "75%          0.000000             0.000000             0.000000   \n",
       "max         40.000000             2.000000            20.000000   \n",
       "\n",
       "       perceived exertion.6  perceived trainingSuccess.6  \\\n",
       "count          42766.000000                 42766.000000   \n",
       "mean               0.247550                     0.349503   \n",
       "std                0.256718                     0.368042   \n",
       "min               -0.010000                    -0.010000   \n",
       "25%               -0.010000                    -0.010000   \n",
       "50%                0.160000                     0.260000   \n",
       "75%                0.440000                     0.720000   \n",
       "max                1.000000                     1.000000   \n",
       "\n",
       "       perceived recovery.6    Athlete ID        injury          Date  \n",
       "count          42766.000000  42766.000000  42766.000000  42766.000000  \n",
       "mean               0.196224     34.550858      0.013632   1228.039892  \n",
       "std                0.190568     19.050033      0.115960    807.021168  \n",
       "min               -0.010000      0.000000      0.000000      0.000000  \n",
       "25%               -0.010000     20.000000      0.000000    436.000000  \n",
       "50%                0.170000     34.000000      0.000000   1256.000000  \n",
       "75%                0.300000     50.000000      0.000000   1913.000000  \n",
       "max                1.000000     73.000000      1.000000   2673.000000  \n",
       "\n",
       "[8 rows x 73 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nr. sessions</th>\n",
       "      <th>nr. rest days</th>\n",
       "      <th>total kms</th>\n",
       "      <th>max km one day</th>\n",
       "      <th>total km Z3-Z4-Z5-T1-T2</th>\n",
       "      <th>nr. tough sessions (effort in Z5, T1 or T2)</th>\n",
       "      <th>nr. days with interval session</th>\n",
       "      <th>total km Z3-4</th>\n",
       "      <th>max km Z3-4 one day</th>\n",
       "      <th>total km Z5-T1-T2</th>\n",
       "      <th>...</th>\n",
       "      <th>max training success.2</th>\n",
       "      <th>avg recovery.2</th>\n",
       "      <th>min recovery.2</th>\n",
       "      <th>max recovery.2</th>\n",
       "      <th>Athlete ID</th>\n",
       "      <th>injury</th>\n",
       "      <th>rel total kms week 0_1</th>\n",
       "      <th>rel total kms week 0_2</th>\n",
       "      <th>rel total kms week 1_2</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>42798.000000</td>\n",
       "      <td>4.279800e+04</td>\n",
       "      <td>4.279800e+04</td>\n",
       "      <td>4.279800e+04</td>\n",
       "      <td>42798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.809337</td>\n",
       "      <td>1.874667</td>\n",
       "      <td>49.543911</td>\n",
       "      <td>14.009255</td>\n",
       "      <td>9.433621</td>\n",
       "      <td>0.930184</td>\n",
       "      <td>1.672531</td>\n",
       "      <td>4.859398</td>\n",
       "      <td>3.456888</td>\n",
       "      <td>4.063970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525709</td>\n",
       "      <td>0.255089</td>\n",
       "      <td>0.184927</td>\n",
       "      <td>0.343522</td>\n",
       "      <td>34.538249</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>4.408628e+05</td>\n",
       "      <td>9.014685e+05</td>\n",
       "      <td>4.803623e+05</td>\n",
       "      <td>1227.733422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.484234</td>\n",
       "      <td>1.853287</td>\n",
       "      <td>36.715017</td>\n",
       "      <td>9.071678</td>\n",
       "      <td>8.887120</td>\n",
       "      <td>1.040631</td>\n",
       "      <td>1.263528</td>\n",
       "      <td>6.984670</td>\n",
       "      <td>4.577423</td>\n",
       "      <td>5.645305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390604</td>\n",
       "      <td>0.153214</td>\n",
       "      <td>0.127996</td>\n",
       "      <td>0.219536</td>\n",
       "      <td>19.020826</td>\n",
       "      <td>0.115130</td>\n",
       "      <td>4.328234e+06</td>\n",
       "      <td>6.945503e+06</td>\n",
       "      <td>4.656806e+06</td>\n",
       "      <td>806.495152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.955245e-01</td>\n",
       "      <td>6.723968e-01</td>\n",
       "      <td>6.923077e-01</td>\n",
       "      <td>437.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.891862e-01</td>\n",
       "      <td>1.001431e+00</td>\n",
       "      <td>9.897523e-01</td>\n",
       "      <td>1254.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>70.100000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.320515e+00</td>\n",
       "      <td>1.422171e+00</td>\n",
       "      <td>1.327586e+00</td>\n",
       "      <td>1913.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>79.800000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.096000e+08</td>\n",
       "      <td>2.176000e+08</td>\n",
       "      <td>2.096000e+08</td>\n",
       "      <td>2673.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nr. sessions  nr. rest days     total kms  max km one day  \\\n",
       "count  42798.000000   42798.000000  42798.000000    42798.000000   \n",
       "mean       5.809337       1.874667     49.543911       14.009255   \n",
       "std        2.484234       1.853287     36.715017        9.071678   \n",
       "min        0.000000       0.000000      0.000000        0.000000   \n",
       "25%        5.000000       1.000000     22.800000        9.000000   \n",
       "50%        6.000000       1.000000     44.800000       13.400000   \n",
       "75%        7.000000       3.000000     70.100000       18.300000   \n",
       "max       14.000000       7.000000    242.000000      131.000000   \n",
       "\n",
       "       total km Z3-Z4-Z5-T1-T2  nr. tough sessions (effort in Z5, T1 or T2)  \\\n",
       "count             42798.000000                                 42798.000000   \n",
       "mean                  9.433621                                     0.930184   \n",
       "std                   8.887120                                     1.040631   \n",
       "min                   0.000000                                     0.000000   \n",
       "25%                   1.000000                                     0.000000   \n",
       "50%                   8.000000                                     1.000000   \n",
       "75%                  14.600000                                     2.000000   \n",
       "max                 100.000000                                     6.000000   \n",
       "\n",
       "       nr. days with interval session  total km Z3-4  max km Z3-4 one day  \\\n",
       "count                    42798.000000   42798.000000         42798.000000   \n",
       "mean                         1.672531       4.859398             3.456888   \n",
       "std                          1.263528       6.984670             4.577423   \n",
       "min                          0.000000       0.000000             0.000000   \n",
       "25%                          0.000000       0.000000             0.000000   \n",
       "50%                          2.000000       0.000000             0.000000   \n",
       "75%                          3.000000       8.000000             6.300000   \n",
       "max                          7.000000      79.800000            75.000000   \n",
       "\n",
       "       total km Z5-T1-T2  ...  max training success.2  avg recovery.2  \\\n",
       "count       42798.000000  ...            42798.000000    42798.000000   \n",
       "mean            4.063970  ...                0.525709        0.255089   \n",
       "std             5.645305  ...                0.390604        0.153214   \n",
       "min             0.000000  ...                0.000000        0.000000   \n",
       "25%             0.000000  ...                0.000000        0.150000   \n",
       "50%             1.500000  ...                0.730000        0.220000   \n",
       "75%             6.300000  ...                0.840000        0.360000   \n",
       "max            80.000000  ...                1.000000        0.900000   \n",
       "\n",
       "       min recovery.2  max recovery.2    Athlete ID        injury  \\\n",
       "count    42798.000000    42798.000000  42798.000000  42798.000000   \n",
       "mean         0.184927        0.343522     34.538249      0.013435   \n",
       "std          0.127996        0.219536     19.020826      0.115130   \n",
       "min          0.000000        0.000000      0.000000      0.000000   \n",
       "25%          0.110000        0.170000     20.000000      0.000000   \n",
       "50%          0.160000        0.310000     34.000000      0.000000   \n",
       "75%          0.240000        0.520000     50.000000      0.000000   \n",
       "max          0.900000        1.000000     73.000000      1.000000   \n",
       "\n",
       "       rel total kms week 0_1  rel total kms week 0_2  rel total kms week 1_2  \\\n",
       "count            4.279800e+04            4.279800e+04            4.279800e+04   \n",
       "mean             4.408628e+05            9.014685e+05            4.803623e+05   \n",
       "std              4.328234e+06            6.945503e+06            4.656806e+06   \n",
       "min              0.000000e+00            0.000000e+00            0.000000e+00   \n",
       "25%              6.955245e-01            6.723968e-01            6.923077e-01   \n",
       "50%              9.891862e-01            1.001431e+00            9.897523e-01   \n",
       "75%              1.320515e+00            1.422171e+00            1.327586e+00   \n",
       "max              2.096000e+08            2.176000e+08            2.096000e+08   \n",
       "\n",
       "               Date  \n",
       "count  42798.000000  \n",
       "mean    1227.733422  \n",
       "std      806.495152  \n",
       "min        0.000000  \n",
       "25%      437.000000  \n",
       "50%     1254.000000  \n",
       "75%     1913.000000  \n",
       "max     2673.000000  \n",
       "\n",
       "[8 rows x 72 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot of two numerical variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='max exertion', y='injury', data=week_ds)\n",
    "plt.title('Scatter Plot of Two Numerical Columns')\n",
    "plt.xlabel('max exertion')\n",
    "plt.ylabel('injury')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high correlation with the target variable:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "max exertion              0.051565\n",
       "avg exertion              0.048771\n",
       "max exertion.1            0.045636\n",
       "avg exertion.1            0.043397\n",
       "max recovery              0.043163\n",
       "max exertion.2            0.040890\n",
       "avg exertion.2            0.038910\n",
       "max training success      0.038485\n",
       "max recovery.1            0.036539\n",
       "max training success.1    0.035923\n",
       "Name: injury, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute correlation coefficients\n",
    "correlation_matrix = week_ds.corr()\n",
    "\n",
    "# Extract correlation coefficients with the target variable\n",
    "correlation_with_target = correlation_matrix[target_variable].drop(target_variable)\n",
    "\n",
    "# Sort the correlation coefficients in descending order\n",
    "correlation_with_target_sorted = correlation_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "# Print the features with high correlation with the target variable\n",
    "print(\"Features with high correlation with the target variable:\")\n",
    "correlation_with_target_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029498342226829264"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_with_target_sorted['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nr. sessions                   0\n",
       "total km                       0\n",
       "km Z3-4                        0\n",
       "km Z5-T1-T2                    0\n",
       "km sprinting                   0\n",
       "                              ..\n",
       "perceived trainingSuccess.6    0\n",
       "perceived recovery.6           0\n",
       "Athlete ID                     0\n",
       "injury                         0\n",
       "Date                           0\n",
       "Length: 73, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of missing data points per column\n",
    "missing_values_count = day_ds.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nr. sessions               0\n",
       "nr. rest days              0\n",
       "total kms                  0\n",
       "max km one day             0\n",
       "total km Z3-Z4-Z5-T1-T2    0\n",
       "                          ..\n",
       "injury                     0\n",
       "rel total kms week 0_1     0\n",
       "rel total kms week 0_2     0\n",
       "rel total kms week 1_2     0\n",
       "Date                       0\n",
       "Length: 72, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of missing data points per column\n",
    "missing_values_count = week_ds.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonic\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.9836 - loss: 0.1229 - val_accuracy: 0.9886 - val_loss: 0.0622\n",
      "Epoch 2/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 26ms/step - accuracy: 0.9872 - loss: 0.0685 - val_accuracy: 0.9886 - val_loss: 0.0623\n",
      "Epoch 3/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0739 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 4/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9861 - loss: 0.0731 - val_accuracy: 0.9886 - val_loss: 0.0623\n",
      "Epoch 5/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9862 - loss: 0.0729 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 6/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9857 - loss: 0.0749 - val_accuracy: 0.9886 - val_loss: 0.0623\n",
      "Epoch 7/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.0728 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 8/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0721 - val_accuracy: 0.9886 - val_loss: 0.0629\n",
      "Epoch 9/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.0747 - val_accuracy: 0.9886 - val_loss: 0.0622\n",
      "Epoch 10/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9861 - loss: 0.0733 - val_accuracy: 0.9886 - val_loss: 0.0624\n",
      "Epoch 11/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9861 - loss: 0.0731 - val_accuracy: 0.9886 - val_loss: 0.0619\n",
      "Epoch 12/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9865 - loss: 0.0712 - val_accuracy: 0.9886 - val_loss: 0.0617\n",
      "Epoch 13/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9860 - loss: 0.0728 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 14/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9867 - loss: 0.0702 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 15/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9850 - loss: 0.0773 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 16/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9853 - loss: 0.0762 - val_accuracy: 0.9886 - val_loss: 0.0617\n",
      "Epoch 17/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.0723 - val_accuracy: 0.9886 - val_loss: 0.0619\n",
      "Epoch 18/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0684 - val_accuracy: 0.9886 - val_loss: 0.0626\n",
      "Epoch 19/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0717 - val_accuracy: 0.9886 - val_loss: 0.0616\n",
      "Epoch 20/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0707 - val_accuracy: 0.9886 - val_loss: 0.0617\n",
      "Epoch 21/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0742 - val_accuracy: 0.9886 - val_loss: 0.0616\n",
      "Epoch 22/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9845 - loss: 0.0792 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 23/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9871 - loss: 0.0689 - val_accuracy: 0.9886 - val_loss: 0.0626\n",
      "Epoch 24/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.0708 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 25/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9865 - loss: 0.0711 - val_accuracy: 0.9886 - val_loss: 0.0624\n",
      "Epoch 26/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0723 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 27/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9850 - loss: 0.0775 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 28/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9870 - loss: 0.0693 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 29/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0682 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 30/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9869 - loss: 0.0689 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 31/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0716 - val_accuracy: 0.9886 - val_loss: 0.0619\n",
      "Epoch 32/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9873 - loss: 0.0677 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 33/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9860 - loss: 0.0729 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 34/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.0697 - val_accuracy: 0.9886 - val_loss: 0.0623\n",
      "Epoch 35/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0730 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 36/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0735 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 37/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9853 - loss: 0.0749 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 38/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0713 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 39/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0729 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 40/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0699 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 41/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.0698 - val_accuracy: 0.9886 - val_loss: 0.0618\n",
      "Epoch 42/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9861 - loss: 0.0718 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 43/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9871 - loss: 0.0674 - val_accuracy: 0.9886 - val_loss: 0.0619\n",
      "Epoch 44/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0669 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 45/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9849 - loss: 0.0764 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 46/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.0719 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 47/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9853 - loss: 0.0754 - val_accuracy: 0.9886 - val_loss: 0.0618\n",
      "Epoch 48/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0717 - val_accuracy: 0.9886 - val_loss: 0.0624\n",
      "Epoch 49/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0707 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 50/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9873 - loss: 0.0668 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 51/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9868 - loss: 0.0689 - val_accuracy: 0.9886 - val_loss: 0.0617\n",
      "Epoch 52/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9861 - loss: 0.0715 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 53/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9875 - loss: 0.0657 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 54/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.0686 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 55/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9861 - loss: 0.0714 - val_accuracy: 0.9886 - val_loss: 0.0618\n",
      "Epoch 56/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0705 - val_accuracy: 0.9886 - val_loss: 0.0616\n",
      "Epoch 57/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0706 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 58/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9869 - loss: 0.0674 - val_accuracy: 0.9886 - val_loss: 0.0619\n",
      "Epoch 59/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9850 - loss: 0.0754 - val_accuracy: 0.9886 - val_loss: 0.0609\n",
      "Epoch 60/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0669 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 61/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9865 - loss: 0.0694 - val_accuracy: 0.9886 - val_loss: 0.0621\n",
      "Epoch 62/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9868 - loss: 0.0679 - val_accuracy: 0.9886 - val_loss: 0.0609\n",
      "Epoch 63/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0670 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 64/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9856 - loss: 0.0723 - val_accuracy: 0.9886 - val_loss: 0.0616\n",
      "Epoch 65/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.0713 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 66/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0702 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 67/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.0681 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 68/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9854 - loss: 0.0722 - val_accuracy: 0.9886 - val_loss: 0.0664\n",
      "Epoch 69/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9873 - loss: 0.0694 - val_accuracy: 0.9886 - val_loss: 0.0618\n",
      "Epoch 70/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9867 - loss: 0.0693 - val_accuracy: 0.9886 - val_loss: 0.0620\n",
      "Epoch 71/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0711 - val_accuracy: 0.9886 - val_loss: 0.0624\n",
      "Epoch 72/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0731 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 73/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0732 - val_accuracy: 0.9886 - val_loss: 0.0610\n",
      "Epoch 74/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9871 - loss: 0.0675 - val_accuracy: 0.9886 - val_loss: 0.0632\n",
      "Epoch 75/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0700 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 76/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0702 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 77/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9860 - loss: 0.0720 - val_accuracy: 0.9886 - val_loss: 0.0615\n",
      "Epoch 78/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9871 - loss: 0.0671 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 79/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0715 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 80/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9867 - loss: 0.0696 - val_accuracy: 0.9886 - val_loss: 0.0614\n",
      "Epoch 81/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9868 - loss: 0.0690 - val_accuracy: 0.9886 - val_loss: 0.0617\n",
      "Epoch 82/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.0713 - val_accuracy: 0.9886 - val_loss: 0.0613\n",
      "Epoch 83/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9862 - loss: 0.0714 - val_accuracy: 0.9886 - val_loss: 0.0610\n",
      "Epoch 84/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.0726 - val_accuracy: 0.9886 - val_loss: 0.0610\n",
      "Epoch 85/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9866 - loss: 0.0693 - val_accuracy: 0.9886 - val_loss: 0.0609\n",
      "Epoch 86/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9859 - loss: 0.0719 - val_accuracy: 0.9886 - val_loss: 0.0608\n",
      "Epoch 87/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9866 - loss: 0.0692 - val_accuracy: 0.9886 - val_loss: 0.0605\n",
      "Epoch 88/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9860 - loss: 0.0714 - val_accuracy: 0.9886 - val_loss: 0.0607\n",
      "Epoch 89/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9856 - loss: 0.0729 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 90/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 37ms/step - accuracy: 0.9863 - loss: 0.0697 - val_accuracy: 0.9886 - val_loss: 0.0607\n",
      "Epoch 91/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - accuracy: 0.9863 - loss: 0.0695 - val_accuracy: 0.9886 - val_loss: 0.0608\n",
      "Epoch 92/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9854 - loss: 0.0733 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 93/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9855 - loss: 0.0724 - val_accuracy: 0.9886 - val_loss: 0.0604\n",
      "Epoch 94/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 37ms/step - accuracy: 0.9861 - loss: 0.0705 - val_accuracy: 0.9886 - val_loss: 0.0603\n",
      "Epoch 95/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 26ms/step - accuracy: 0.9872 - loss: 0.0654 - val_accuracy: 0.9886 - val_loss: 0.0605\n",
      "Epoch 96/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.0705 - val_accuracy: 0.9886 - val_loss: 0.0608\n",
      "Epoch 97/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9861 - loss: 0.0695 - val_accuracy: 0.9886 - val_loss: 0.0606\n",
      "Epoch 98/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9863 - loss: 0.0687 - val_accuracy: 0.9886 - val_loss: 0.0612\n",
      "Epoch 99/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9867 - loss: 0.0670 - val_accuracy: 0.9886 - val_loss: 0.0606\n",
      "Epoch 100/100\n",
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0649 - val_accuracy: 0.9886 - val_loss: 0.0604\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.07248784601688385\n",
      "Test Accuracy: 0.9864485859870911\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Load the dataset from CSV\n",
    "df = week_ds\n",
    "\n",
    "# Use all features except 'injury' as input\n",
    "input_data = df.drop(columns=['injury']).values\n",
    "\n",
    "# Normalize the input data (optional but recommended)\n",
    "input_data = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "\n",
    "# Use 'injury' column as the target variable y\n",
    "y = df['injury'].values\n",
    "\n",
    "# Reshape the input data for LSTM (samples, timesteps, features)\n",
    "x_tf = np.expand_dims(input_data, axis=-1)\n",
    "y_tf = y\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_tf, y_tf, test_size=0.2, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define and compile the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=64, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    Dense(units=1, activation='sigmoid')  # Output layer for binary classification (sigmoid activation)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
